# -*- coding: utf-8 -*-
"""MLlab02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CznWerkRrDl22Q54rOkQASa1Qd2IUh40
"""

import numpy as np
import matplotlib.pyplot as plt

NumDataPerClass = 500
# Two-class problem, distinct means, equal covariance matrices
#
m1 = [[0, 5]]
m2 = [[5, 0]]
C = [[2, 1], [1, 2]]
# Set up the data by generating isotropic Guassians and
# rotating them accordingly
#
A = np.linalg.cholesky(C)
U1 = np.random.randn(NumDataPerClass,2)
X1 = U1 @ A.T + m1
U2 = np.random.randn(NumDataPerClass,2)
X2 = U2 @ A.T + m2

print(np.shape(X1))
print(np.shape(X2))

fig,ax=plt.subplots(figsize=(6,6))
ax.scatter(X1[:,0],X1[:,1],c="c",s=8)
ax.scatter(X2[:,0],X2[:,1],c="m",s=8)
ax.set_xlim(-6,10)
ax.set_ylim(-6,10)

#Q2
X = np.concatenate((X1, X2), axis=0)
print(np.shape(X))

#Q3
labelPos = np.ones(NumDataPerClass)
labelNeg =-1.0 * np.ones(NumDataPerClass)
y = np.concatenate((labelPos, labelNeg))
print(np.shape(y))

#Q4
rIndex = np.random.permutation(2*NumDataPerClass)
Xr = X[rIndex,]
yr = y[rIndex]
# Training and test sets (half half)
#
X_train = Xr[0:NumDataPerClass]
y_train = yr[0:NumDataPerClass]
X_test = Xr[NumDataPerClass:2*NumDataPerClass]
y_test = yr[NumDataPerClass:2*NumDataPerClass]
print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)
Ntrain = NumDataPerClass;
Ntest = NumDataPerClass;

#Q5
def PercentCorrect(Inputs, targets, weights):
  N = len(targets)
  nCorrect = 0
  for n in range(N):
    OneInput = Inputs[n,:]
    if (targets[n] * np.dot(OneInput, weights) > 0):
      nCorrect +=1
  return 100*nCorrect/N

#Q6
# Perceptron learning loop
#
# Random initialization of weights
#
w = np.random.randn(2)
print(w)
# What is the performance with the initial random weights?
#
print('Initial Percentage Correct: %6.2f' %(PercentCorrect(X_train, y_train, w)))
# Fixed number of iterations (think of better stopping criterion)
#
MaxIter=1000
# Learning rate (change this to see convergence changing)
#
alpha = 0.002
# Space to save answers for plotting
#
P_train = np.zeros(MaxIter)
P_test = np.zeros(MaxIter)
# Main Loop
#
for iter in range(MaxIter):
 # Select a data item at random
 #
 r = np.floor(np.random.rand()*Ntrain).astype(int)
 x = X_train[r,:]
 # If it is misclassified, update weights
 #
 if (y_train[r] * np.dot(x, w) < 0):
  w += alpha * y_train[r] * x
 # Evaluate trainign and test performances for plotting
 #
 P_train[iter] = PercentCorrect(X_train, y_train, w);
 P_test[iter] = PercentCorrect(X_test, y_test, w);
print('Percentage Correct After Training: %6.2f %6.2f' %(PercentCorrect(X_train, y_train, w), PercentCorrect(X_test, y_test, w)))

#Q7
fig, ax = plt.subplots(figsize=(6,4))
ax.plot(range(MaxIter), P_train, 'b', label = "Training")
ax.plot(range(MaxIter), P_test, 'r', label = "Test")
ax.grid(True)
ax.legend()
ax.set_title('Perceptron Learning')
ax.set_ylabel('Training and Test Accuracies', fontsize=14)
ax.set_xlabel('Iteration', fontsize=14)
plt.savefig('learningCurves.png')

#Q8
# Scikitlearn can do it for us
#
from sklearn.linear_model import Perceptron
from sklearn.metrics import accuracy_score
model = Perceptron()
model.fit(X_train, y_train)
yh_train = model.predict(X_train)
print("Accuracy on training set: %6.2f" %(accuracy_score(yh_train, y_train)))

yh_test = model.predict(X_test)
print("Accuracy on test set: %6.2f" %(accuracy_score(yh_test, y_test)))

if (accuracy_score(yh_test, y_test) > 0.99):
  print("Wow, Perfect Classification on Separable dataset!")

#Q9
import numpy as np
import matplotlib.pyplot as plt

NumDataPerClass_2 = 50
# Two-class problem, distinct means, equal covariance matrices
#
m1_2 = [[2.5, 2.5]]
m2_2 = [[10.0, 10.0]]
C_2 = [[2, 1], [1, 2]]
# Set up the data by generating isotropic Guassians and
# rotating them accordingly
#
A_2 = np.linalg.cholesky(C_2)
U1_2 = np.random.randn(NumDataPerClass_2,2)
X1_2 = U1_2 @ A_2.T + m1_2
U2_2 = np.random.randn(NumDataPerClass_2,2)
X2_2 = U2_2 @ A_2.T + m2_2

X_2 = np.concatenate((X1_2, X2_2), axis=0)

labelPos_2 = np.ones(NumDataPerClass_2)
labelNeg_2 =-1.0 * np.ones(NumDataPerClass_2)
y_2 = np.concatenate((labelPos_2, labelNeg_2))

rIndex_2 = np.random.permutation(2*NumDataPerClass_2)
Xr_2 = X_2[rIndex_2,]
yr_2 = y_2[rIndex_2]
# Training and test sets (half half)
#

X_train_2 = Xr_2[0:NumDataPerClass_2]
y_train_2 = yr_2[0:NumDataPerClass_2]
X_test_2 = Xr_2[NumDataPerClass_2:2*NumDataPerClass_2]
y_test_2 = yr_2[NumDataPerClass_2:2*NumDataPerClass_2]
print(X_train_2.shape, y_train_2.shape, X_test_2.shape, y_test_2.shape)
Ntrain = NumDataPerClass_2;
Ntest = NumDataPerClass_2;

# Scikitlearn can do it for us
#
from sklearn.linear_model import Perceptron
from sklearn.metrics import accuracy_score
model = Perceptron()
model.fit(X_train_2, y_train_2)
yh_train_2 = model.predict(X_train_2)
print("Accuracy on training set: %6.2f" %(accuracy_score(yh_train_2, y_train_2)))

yh_test_2 = model.predict(X_test_2)
print("Accuracy on test set: %6.2f" %(accuracy_score(yh_test_2, y_test_2)))

if (accuracy_score(yh_test_2, y_test_2) > 0.99):
  print("Wow, Perfect Classification on Separable dataset!")
